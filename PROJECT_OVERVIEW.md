# Reasoning Direction Project - Overview

This project successfully recreates the research code for exploring "reasoning directions" in LLM activation space, reproducing experiments from a paper that demonstrated bidirectional control over reasoning behavior in models like DeepSeek R1.

## ‚úÖ What Was Built

### Core Modules (`src/`)

1. **`config.py`**: Configuration management
   - Model configurations (Llama-3 8B, DeepSeek R1)
   - Data configurations (GSM8K, toy problems)
   - Experiment parameters (layers, strengths, activation points)
   - 10 predefined toy math problems

2. **`model_loader.py`**: Model loading
   - HookedTransformer integration
   - Support for both reasoning and non-reasoning models
   - Automatic device and dtype management

3. **`data_loader.py`**: Dataset handling
   - GSM8K mathematical reasoning problems
   - Toy problems for quick testing
   - Chat template formatting for each model

4. **`activation_extractor.py`**: Activation extraction
   - Hook into residual stream at pre/mid/post attention points
   - Batch processing for efficiency
   - Mean activation calculation

5. **`direction_calculator.py`**: Direction computation
   - Difference directions (reasoning - non-reasoning)
   - Original directions (non-reasoning only)
   - Reasoning directions (reasoning only)
   - Random directions (control experiments)
   - Normalization to unit vectors

6. **`intervention.py`**: Activation interventions
   - Hook-based intervention during generation
   - Parametric strength control
   - Comprehensive sweep functionality
   - Result tracking (output, token count)

7. **`experiment.py`**: Orchestration
   - Full pipeline: load ‚Üí extract ‚Üí calculate ‚Üí intervene
   - Automatic result saving with timestamps
   - Support for testing on both model types

### Scripts & Examples

1. **`run_experiment.py`**: Main CLI interface
   - Comprehensive argument parsing
   - Flexible experiment configuration
   - Production-ready execution

2. **`examples/quick_test.py`**: Fast testing script
   - Minimal compute requirements
   - 5-10 minute runtime
   - Good for development

3. **`examples/interactive_demo.ipynb`**: Jupyter notebook
   - Step-by-step walkthrough
   - Visual exploration of results
   - Educational resource

4. **`examples/analyze_results.py`**: Results analysis
   - Token count analysis
   - Reasoning suppression detection
   - Top intervention identification

### Documentation

1. **`README.md`**: Project introduction
   - Background and methodology
   - Installation instructions
   - Hardware requirements
   - Expected results

2. **`USAGE.md`**: Comprehensive usage guide
   - Quick start examples
   - Command-line options
   - Common workflows
   - Troubleshooting

3. **`PROJECT_OVERVIEW.md`** (this file): Architecture overview

### Configuration

1. **`pyproject.toml`**: Package management
   - uv-compatible configuration
   - All dependencies specified
   - Dev dependencies included

2. **`.gitignore`**: Version control
   - Excludes models, data, results
   - Python-specific ignores

## üéØ Key Features Implemented

### 1. Activation Extraction
- Three extraction points per layer (pre/mid/post attention)
- Efficient batch processing
- Support for all 31 layers in 8B models

### 2. Direction Calculation
- **Difference direction**: Core reasoning direction (reasoning - non-reasoning)
- **Control directions**: Original, reasoning-only, random
- Normalized unit vectors for consistent strength application

### 3. Interventions
- Bidirectional control (positive/negative strengths)
- Layer-specific targeting
- Real-time generation modification
- Comprehensive parameter sweeps

### 4. Experiment Pipeline
```
Load Models ‚Üí Extract Activations ‚Üí Calculate Directions ‚Üí Run Interventions ‚Üí Save Results
```

### 5. Results Management
- Timestamped outputs
- JSON format for intervention results
- PyTorch format for activations/directions
- Easy result loading for further analysis

## üìä Expected Experimental Findings

Based on the original paper, you should observe:

### On Reasoning Model (DeepSeek R1)

**Enhanced Reasoning** (positive strength, e.g., +0.1):
- Increased token count (often 100+ more tokens)
- More verbose reasoning in `<think>` tags
- Hesitant, uncertain behavior
- Multiple reconsiderations and backtracking
- At very high strengths (0.2+): degeneration to repetitive `<think>`

**Suppressed Reasoning** (negative strength, e.g., -0.1):
- Decreased token count
- Elimination of `<think>` sections
- Direct answers without explicit reasoning
- Most effective in layer 0

**Layer Specificity**:
- Strongest effects in early layers (0-3)
- Diminishing effects in later layers (19+)

### On Non-Reasoning Model (Llama-3 8B)

- Generally maintains typical response patterns
- Interventions don't induce explicit reasoning behavior
- Suggests reasoning requires more than single-direction modification

## üöÄ Quick Start Commands

```bash
# Install dependencies
uv sync

# Quick test (5-10 minutes)
uv run python examples/quick_test.py

# Full experiment (30-60 minutes)
uv run python run_experiment.py

# Interactive exploration
uv run jupyter lab
# Open examples/interactive_demo.ipynb

# Analyze results
uv run python examples/analyze_results.py results/interventions_reasoning_*.json
```

## üèóÔ∏è Architecture Decisions

### Why HookedTransformer?
- Direct access to intermediate activations
- Easy hook registration
- Compatible with HuggingFace models

### Why Three Activation Points?
- Pre: Before attention (purest residual stream)
- Mid: After attention, before MLP (attention effects)
- Post: After full block (combined effects)

### Why Normalize Directions?
- Consistent strength interpretation across layers
- Fair comparison between direction types
- Prevents magnitude-based artifacts

### Why Save Intermediate Results?
- Avoid recomputation (expensive)
- Enable multiple intervention experiments
- Support iterative analysis

## üìà Recommended Experiment Progression

1. **Phase 1: Validation** (Quick test)
   ```bash
   uv run python run_experiment.py --dataset toy --layers 0 1 2
   ```

2. **Phase 2: Key Finding** (Layer 0 focus)
   ```bash
   uv run python run_experiment.py --layers 0 --strengths -0.2 -0.15 -0.1 -0.05 0.05 0.1 0.15 0.2
   ```

3. **Phase 3: Layer Sweep** (Find other interesting layers)
   ```bash
   uv run python run_experiment.py --layers 0 1 2 3 5 7 10 12 15 17 20 22 25 27 30
   ```

4. **Phase 4: Comprehensive** (Full characterization)
   ```bash
   uv run python run_experiment.py --test-both --points pre mid post
   ```

## üî¨ Extension Ideas

1. **New Direction Calculation Methods**
   - PCA on activation differences
   - Supervised direction finding
   - Layer-wise difference (not just mean)

2. **Advanced Interventions**
   - Multi-layer simultaneous intervention
   - Dynamic strength based on token
   - Gradient-based direction refinement

3. **Additional Analysis**
   - Cosine similarity between directions across layers
   - Activation trajectory visualization
   - Attention pattern changes during intervention

4. **Other Models**
   - Test on different model sizes (70B, etc.)
   - Test on other reasoning models (o1-mini, etc.)
   - Test on different base models

5. **Other Tasks**
   - Code generation
   - Logical reasoning (LOGIC, etc.)
   - Creative writing

## üõ†Ô∏è Code Quality Features

- **Type hints**: Throughout for clarity
- **Docstrings**: All classes and methods documented
- **Modular design**: Easy to extend and modify
- **Configuration-driven**: No hardcoded values
- **Error handling**: Graceful degradation
- **Progress tracking**: tqdm for long operations
- **Logging**: Clear progress messages

## üì¶ Dependencies

Core:
- `torch`: Deep learning framework
- `transformer-lens`: HookedTransformer implementation
- `transformers`: HuggingFace models
- `datasets`: GSM8K and other datasets

Utilities:
- `numpy`, `pandas`: Data manipulation
- `tqdm`: Progress bars
- `einops`: Tensor operations

Development:
- `jupyter`: Interactive notebooks
- `ipython`: Enhanced REPL

## üéì Learning Outcomes

After using this project, you'll understand:

1. How to extract and manipulate LLM activations
2. How to compute meaningful directions in activation space
3. How to intervene on model behavior in real-time
4. The architecture of reasoning models
5. The layer-wise organization of model computations
6. Experimental design for mechanistic interpretability

## üìù Citation

If you use this code for research, please cite the original paper:
```
Arditi, A., et al. (2024). Refusal in Large Language Models as Mediated by a Single Direction.
```

## ü§ù Contributing

This is a research reproduction project. Contributions welcome:
- Bug fixes
- Performance improvements
- Documentation enhancements
- New analysis methods
- Additional model support

## üìÑ License

MIT License - See project root for details.

---

**Status**: ‚úÖ Complete and ready to use

**Last Updated**: November 29, 2025

**Maintainer**: Research reproduction project

